{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d30ffa1",
   "metadata": {},
   "source": [
    "# 18.06 Problem Set 10 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8381feb2",
   "metadata": {},
   "source": [
    "## Problem 1 (5+5+5+5 points)\n",
    "\n",
    "In class, we saw that $o = [1,1,\\ldots,1,1]$ is an eigenvector of $M^T$ with eigenvalue $\\lambda = 1$ for any Markov matrix $M$.\n",
    "\n",
    "**(a)** If $x_k$ is an eigenvector of $M$ ($M x_k = \\lambda_k x_k$) for any *other* eigenvalue $\\lambda_k \\ne 1$ of $M$, show that we must have $o^T x_k = 0$: it must be *orthogonal* to $o$.  (Hint: use $o^T = o^T M$.)\n",
    "\n",
    "**(b)** Check your result from (a) numerically for a random $5 \\times 5$ Markov matrix `M = rand(5,5); M = M ./ sum(M, dims=1)`, with eigenvalues `eigvals(M)` and eigenvectors `X = eigvecs(M)`.   (Do `using LinearAlgebra` to get `eigvecs` and `eigvals`.)\n",
    "\n",
    "(Note: if you have a long vector `v`, Julia only shows a few elements by default, but you can show all the elements with `@show v`.  You can also look at the absolute values of the elements with `abs.(v)`, which can be easier to read than complex numbers in checking that entries are small.)\n",
    "\n",
    "**(c)** If we expand an arbitrary $x$ in an eigenvector basis $x = c_1 x_1 + \\cdots + c_m x_m$, letting $x_m$ be a steady-state eigenvector ($\\lambda_m = 1$) and supposing all of the other eigenvalues are $\\ne 1$, show that $o^T x$ gives us a simple formula for $c_m = \\_\\_\\_\\_\\_\\_\\_\\_$.\n",
    "\n",
    "**(d)** Hence, if all other eigenvalues have magnitude $<1$, then $M^n x \\to \\_\\_\\_\\_\\_\\_\\_\\_$ (simple formula in $o,x,x_m$) as $n \\to \\infty$.   Check this formula against `M^100 * [1,2,3,4,5]` for your `M` from (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac515620",
   "metadata": {},
   "source": [
    "## Solution 1\n",
    "\n",
    "**(a)** As $M$ is Markov, we have $o^T = o^TM$. Let us multiply both sides of $o^T = o^TM$ against the vector $x_k$, where $x_k$ is an eigenvector such that $Mx_k = \\lambda_k x_k$ and $\\lambda_k \\neq 1$. We have\n",
    "$$o^Tx_k = o^TMx_k$$\n",
    "$$\\implies o^Tx_k = \\lambda_k o^Tx_k$$\n",
    "$$\\implies (1 - \\lambda_k) o^Tx_k = 0$$\n",
    "$$\\implies o^T x_k = 0,$$\n",
    "where the last implication follows since $\\lambda_k \\neq 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd026d20",
   "metadata": {},
   "source": [
    "**(b)** The code is below, and indeed the dot products $o^T X$ are $\\approx 0$ (up to roundoff errors) except for the (last) eigenvector corresponding to $\\lambda = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5197e59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ = ComplexF64[-0.025239843953041324 - 0.17137257909892123im, -0.025239843953041324 + 0.17137257909892123im, 0.0569700076493306 + 0.0im, 0.1222868283145958 + 0.0im, 1.0000000000000002 + 0.0im]\n",
      "abs.(result) = [4.726604209672303e-16 4.726604209672303e-16 4.440892098500626e-16 2.3592239273284576e-16 2.154503435713057]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1×5 Matrix{Float64}:\n",
       " 4.7266e-16  4.7266e-16  4.44089e-16  2.35922e-16  2.1545"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra;\n",
    "\n",
    "# generate Markov matrix \n",
    "M = rand(5, 5);\n",
    "M = M ./ sum(M, dims=1);\n",
    "\n",
    "# compute eigenvalues and eigenvectors\n",
    "λ = eigvals(M);\n",
    "X = eigvecs(M);\n",
    "\n",
    "# check that o^T x_k is 0 for eigenvectors whose corresponding eigenvalue is not 1\n",
    "o = [1, 1, 1, 1, 1];\n",
    "result = o' * X;\n",
    "@show λ\n",
    "@show abs.(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14340d21",
   "metadata": {},
   "source": [
    "**(c)** Taking the dot product of $o$ with $x$ gives\n",
    "$$o^Tx = o^T(c_1x_1 + c_2x_2 + \\ldots + c_mx_m)$$\n",
    "$$= c_1o^Tx_1 + c_2o^Tx_2 + \\ldots + c_mo^Tx_m.$$\n",
    "From (a), we know that $o^Tx_i = 0$ for $i \\neq m$.\n",
    "Thus, we have $$o^Tx = c_mo^Tx_m \\implies c_m = \\frac{o^Tx}{o^Tx_m}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51efd858",
   "metadata": {},
   "source": [
    "**(d)** We have $M^nx = c_1 \\lambda_1^n x_1 + \\ldots + c_m \\lambda_m^n x_m$. As $n\\to \\infty$, $\\lambda_i^n \\to 0$ for $i \\neq m$. So, $$M^nx \\to c_m \\lambda_m^n x_m = \\frac{o^Tx}{o^Tx_m} x_m$$\n",
    "where we use the formula for $c_m$ from (c). The code below observes this convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f640df11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xTrue = [1.9338510947374465, 3.8824772333448783, 3.9405657589028347, 3.0603805344264408, 2.182725378588311]\n",
      "xExpected = ComplexF64[1.9338510947374572 + 0.0im, 3.8824772333449022 + 0.0im, 3.9405657589028564 + 0.0im, 3.0603805344264576 + 0.0im, 2.1827253785883243 + 0.0im]\n",
      "abs.(xTrue - xExpected) = [1.0658141036401503e-14, 2.398081733190338e-14, 2.1760371282653068e-14, 1.687538997430238e-14, 1.3322676295501878e-14]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       " 1.0658141036401503e-14\n",
       " 2.398081733190338e-14\n",
       " 2.1760371282653068e-14\n",
       " 1.687538997430238e-14\n",
       " 1.3322676295501878e-14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 2, 3, 4, 5];\n",
    "\n",
    "# calculate true result\n",
    "xTrue = M^100 * x;\n",
    "# calculate expected result from part (c)'s work\n",
    "xExpected = ((o' * x) / (o' * X[:,5])) .* X[:,5];\n",
    "\n",
    "# compare the two\n",
    "@show xTrue\n",
    "@show xExpected\n",
    "@show abs.(xTrue - xExpected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34993f46",
   "metadata": {},
   "source": [
    "## Problem 2 (10+5 points)\n",
    "\n",
    "From Strang, section 6.2.  Consider $A = \\begin{pmatrix} 0.6 & 0.4 \\\\ 0.4 & 0.6 \\end{pmatrix}$ and $B = \\begin{pmatrix} 0.6 & 0.9 \\\\ 0.1 & 0.6 \\end{pmatrix}$.   For this problem you keep in mind the diagonalization of matrices like $A$ and $B$.\n",
    "\n",
    "**(a)** Which of $A^n$ or $B^n$ (or both, or neither) go $\\to \\begin{pmatrix}0 & 0 \\\\ 0 & 0 \\end{pmatrix}$ as $n \\to \\infty$?   Double-check your answer by looking at $A^{100}$ and $B^{100}$ in Julia — these are approaching matrices of rank \\_\\_\\_\\_\\_\\_\\_\\_ and \\_\\_\\_\\_\\_\\_\\_\\_, respectively.\n",
    "\n",
    "**(b)** For what values of the real scalar $\\mu$ is $\\sqrt{A - \\mu I}$ a real matrix?  Check your answer by trying `sqrt(A - μ*Ι)` for a few values of `μ` in Julia (do `using LinearAlgebra` to get `I`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc24f2c4",
   "metadata": {},
   "source": [
    "## Solution 2\n",
    "\n",
    "**(a)** $A$ is a positive Markov matrix, so it must have one eigenvalue $\\lambda_1 = 1$ and another eigenvalue with magnitude $|\\lambda_2| < 1$.  So, with *no calculation*, we can see that we must have\n",
    "$$A^n = X\\begin{pmatrix}\\lambda_1 &\\\\&\\lambda_2\\end{pmatrix}^nX^{-1} = X\\begin{pmatrix}\\lambda_1^n&\\\\&\\lambda_2^n\\end{pmatrix}X^{-1} \\to X\\begin{pmatrix}1&\\\\&1\\end{pmatrix}X^{-1}.$$\n",
    "As one of the diagonal entries is zero, $A^n$ is approaching a matrix of $\\boxed{\\text{rank } 1}$ (and so not going to $\\begin{pmatrix}0&0\\\\0&0\\end{pmatrix}$)\n",
    "\n",
    "We can compute the eigenvalues explicitly, of course.  From the quadratic formula for $\\det(A-\\lambda I) = \\lambda^2 - \\mathrm{trace}(A) \\lambda + \\det A$, we get $\\lambda = 0.6 \\pm \\sqrt{0.6^2 - (0.6^2 - 0.4^2)} = 0.6 \\pm 0.4 = 1 \\textrm{ or } 0.2$.  We can also check this in Julia below. So the diagonalization of $A$ is $X\\begin{pmatrix}1&\\\\&0.2\\end{pmatrix}X^{-1}$ where $X$ is the matrix of $A$'s eigenvectors. As $n \\to \\infty$, we have $$A^n = X\\begin{pmatrix}1&\\\\&0.2\\end{pmatrix}^nX^{-1} = X\\begin{pmatrix}1^n&\\\\&0.2^n\\end{pmatrix}X^{-1} \\to X\\begin{pmatrix}1&\\\\&0\\end{pmatrix}X^{-1}.$$\n",
    "As one of the diagonal entries is zero, $A^n$ is approaching a matrix of $\\boxed{\\text{rank } 1}$ (and so not going to $\\begin{pmatrix}0&0\\\\0&0\\end{pmatrix}$).\n",
    "\n",
    "$B$ is not a Markov matrix, so we'll just calculate the eigenvalues explicitly.  Via the quadratic formula, we get $\\lambda = 0.6 \\pm \\sqrt{0.6^2 - (0.6^2 - 0.9 \\times 0.1)} = 0.6 \\pm \\sqrt{0.09} = 0.6 \\pm 0.3 = 0.3 \\textrm{ or } 0.9$ So the diagonalization of $B$ is $X\\begin{pmatrix}0.3&\\\\&0.9\\end{pmatrix}X^{-1}$ where $X$ is the matrix of $B$'s eigenvectors. As $n \\to \\infty$, we have $$B^n = X\\begin{pmatrix}0.3&\\\\&0.9\\end{pmatrix}^nX^{-1} = X\\begin{pmatrix}0.3^n&\\\\&0.9^n\\end{pmatrix}X^{-1} \\to X\\begin{pmatrix}0&\\\\&0\\end{pmatrix}X^{-1}.$$\n",
    "As both of the diagonal entries are zero, $B^n$ is approaching a matrix of $\\boxed{\\text{rank } 0}$, and so must be going to $\\begin{pmatrix}0&0\\\\0&0\\end{pmatrix}$.\n",
    "\n",
    "The code below confirms this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b2a18a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ_A = [0.19999999999999996, 1.0]\n",
      "λ_B = [0.29999999999999993, 0.8999999999999999]\n",
      "A ^ 100 = [0.5 0.5; 0.5 0.5]\n",
      "B ^ 100 = [1.3280699443793745e-5 3.984209833138124e-5; 4.426899814597916e-6 1.3280699443793748e-5]\n"
     ]
    }
   ],
   "source": [
    "A = [0.6 0.4; 0.4 0.6];\n",
    "B = [0.6 0.9; 0.1 0.6];\n",
    "\n",
    "using LinearAlgebra\n",
    "\n",
    "# compute eigenvalues\n",
    "λ_A = eigvals(A);\n",
    "λ_B = eigvals(B);\n",
    "@show λ_A\n",
    "@show λ_B\n",
    "\n",
    "# check high powers\n",
    "@show A^100;\n",
    "@show B^100;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6871a99",
   "metadata": {},
   "source": [
    "**(b)** Note that the eigenvalues of $A - \\mu I$ are $0.2 - \\mu$ and $1.0 - \\mu$, with the same corresponding eigenvectors as $A$. Then $A - \\mu I = X\\begin{pmatrix}0.2 - \\mu&\\\\&1.0 - \\mu\\end{pmatrix}X^{-1}$ and in particular, $$\\sqrt{A - \\mu I} = X\\begin{pmatrix}0.2 - \\mu&\\\\&1.0 - \\mu\\end{pmatrix}^{\\frac{1}{2}}X^{-1} = X\\begin{pmatrix}\\sqrt{0.2 - \\mu}&\\\\&\\sqrt{1.0 - \\mu}\\end{pmatrix}X^{-1}.$$\n",
    "Note that $X$ and $X^{-1}$ are real matrices. For $\\sqrt{A - \\mu I}$ to be a real matrix, we thus need $\\sqrt{0.2 - \\mu}$ and $\\sqrt{1.0 - \\mu}$ to be real, or equivalently, $\\mu \\le 0.2$ and $\\mu \\le 1.0$. So the answer is $\\boxed{\\mu \\le 0.2}$.\n",
    "\n",
    "The code below confirms this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37cd5be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "μ: 0.0\n",
      "[0.7236067977499787 0.276393202250021; 0.276393202250021 0.7236067977499787]\n",
      "μ: 0.1\n",
      "[0.6324555320336757 0.3162277660168379; 0.3162277660168379 0.6324555320336757]\n",
      "μ: 0.2\n",
      "[0.4472135954999578 0.4472135954999578; 0.4472135954999578 0.4472135954999578]\n",
      "μ: 0.3\n",
      "ComplexF64[0.4183300132670377 + 0.15811388300841897im 0.4183300132670377 - 0.15811388300841897im; 0.4183300132670377 - 0.15811388300841897im 0.4183300132670377 + 0.15811388300841897im]\n",
      "μ: 0.4\n",
      "ComplexF64[0.38729833462074165 + 0.223606797749979im 0.38729833462074165 - 0.223606797749979im; 0.38729833462074165 - 0.223606797749979im 0.38729833462074165 + 0.223606797749979im]\n",
      "μ: 0.5\n",
      "ComplexF64[0.35355339059327373 + 0.27386127875258304im 0.35355339059327373 - 0.27386127875258304im; 0.35355339059327373 - 0.27386127875258304im 0.35355339059327373 + 0.27386127875258304im]\n",
      "μ: 0.6\n",
      "ComplexF64[0.3162277660168379 + 0.3162277660168379im 0.3162277660168379 - 0.3162277660168379im; 0.3162277660168379 - 0.3162277660168379im 0.3162277660168379 + 0.3162277660168379im]\n",
      "μ: 0.7\n",
      "ComplexF64[0.27386127875258304 + 0.35355339059327373im 0.27386127875258304 - 0.35355339059327373im; 0.27386127875258304 - 0.35355339059327373im 0.27386127875258304 + 0.35355339059327373im]\n",
      "μ: 0.8\n",
      "ComplexF64[0.22360679774997888 + 0.38729833462074165im 0.22360679774997888 - 0.38729833462074165im; 0.22360679774997888 - 0.38729833462074165im 0.22360679774997888 + 0.38729833462074165im]\n",
      "μ: 0.9\n",
      "ComplexF64[0.15811388300841892 + 0.4183300132670377im 0.15811388300841892 - 0.4183300132670377im; 0.15811388300841892 - 0.4183300132670377im 0.15811388300841892 + 0.4183300132670377im]\n",
      "μ: 1.0\n",
      "ComplexF64[0.0 + 0.4472135954999578im 0.0 - 0.4472135954999578im; 0.0 - 0.4472135954999578im 0.0 + 0.4472135954999578im]\n"
     ]
    }
   ],
   "source": [
    "# test μ from 0 to 1 in 0.1 increments\n",
    "for μ in LinRange(0.0, 1.0, 11)\n",
    "    X = sqrt(A - μ * I);\n",
    "    println(\"μ: \", μ);\n",
    "    println(X);\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
